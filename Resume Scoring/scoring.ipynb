{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based Scoring using the NER model trained using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from tika import parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "ner_model = spacy.load(os.path.join(os.path.dirname(os.getcwd()),\"E:\\work\\Daily/3_6\\Resume-Summarization-and-Scoring-main\\Training_NER\\model-last\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a test resume and the JD of Borneo for testing out the scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 08:40:32,143 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to C:\\Users\\YGP\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2023-03-08 09:10:21,274 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to C:\\Users\\YGP\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2023-03-08 09:10:23,095 [MainThread  ] [ERROR]  Unable to run java; is it installed?\n",
      "2023-03-08 09:10:23,096 [MainThread  ] [ERROR]  Failed to receive startup confirmation from startServer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to start Tika server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser_jd \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mfrom_file(\u001b[39m\"\u001b[39;49m\u001b[39mborneo-JD.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m parser_resume \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mfrom_file(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mgetcwd()),\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mTraining_NER/dataset/test/chenna kesava.docx\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m jd \u001b[39m=\u001b[39m parser_jd[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\parser.py:40\u001b[0m, in \u001b[0;36mfrom_file\u001b[1;34m(filename, serverEndpoint, service, xmlContent, headers, config_path, requestOptions, raw_response)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mParses a file for metadata and content\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m:param filename: path to file which needs to be parsed or binary file using open(path,'rb')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m        'content' has a str value and metadata has a dict type value.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xmlContent:\n\u001b[1;32m---> 40\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, headers\u001b[39m=\u001b[39;49mheaders, config_path\u001b[39m=\u001b[39;49mconfig_path, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, services\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/meta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/tika\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/rmeta/xml\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m     43\u001b[0m                         headers\u001b[39m=\u001b[39mheaders, config_path\u001b[39m=\u001b[39mconfig_path, requestOptions\u001b[39m=\u001b[39mrequestOptions)\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:337\u001b[0m, in \u001b[0;36mparse1\u001b[1;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    335\u001b[0m headers\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mAccept\u001b[39m\u001b[39m'\u001b[39m: responseMimeType, \u001b[39m'\u001b[39m\u001b[39mContent-Disposition\u001b[39m\u001b[39m'\u001b[39m: make_content_disposition_header(path\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39mis\u001b[39;00m unicode_string \u001b[39melse\u001b[39;00m path)})\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m urlOrPath \u001b[39mif\u001b[39;00m _is_file_object(urlOrPath) \u001b[39melse\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 337\u001b[0m     status, response \u001b[39m=\u001b[39m callServer(\u001b[39m'\u001b[39;49m\u001b[39mput\u001b[39;49m\u001b[39m'\u001b[39;49m, serverEndpoint, service, f,\n\u001b[0;32m    338\u001b[0m                                   headers, verbose, tikaServerJar, config_path\u001b[39m=\u001b[39;49mconfig_path,\n\u001b[0;32m    339\u001b[0m                                   rawResponse\u001b[39m=\u001b[39;49mrawResponse, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m    341\u001b[0m \u001b[39mif\u001b[39;00m file_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39munlink(path)\n\u001b[0;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m (status, response)\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:532\u001b[0m, in \u001b[0;36mcallServer\u001b[1;34m(verb, serverEndpoint, service, data, headers, verbose, tikaServerJar, httpVerbs, classpath, rawResponse, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39mglobal\u001b[39;00m TikaClientOnly\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TikaClientOnly:\n\u001b[1;32m--> 532\u001b[0m     serverEndpoint \u001b[39m=\u001b[39m checkTikaServer(scheme, serverHost, port, tikaServerJar, classpath, config_path)\n\u001b[0;32m    534\u001b[0m serviceUrl  \u001b[39m=\u001b[39m serverEndpoint \u001b[39m+\u001b[39m service\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m verb \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m httpVerbs:\n",
      "File \u001b[1;32mc:\\Users\\YGP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tika\\tika.py:602\u001b[0m, in \u001b[0;36mcheckTikaServer\u001b[1;34m(scheme, serverHost, port, tikaServerJar, classpath, config_path)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m status:\n\u001b[0;32m    601\u001b[0m             log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mFailed to receive startup confirmation from startServer.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to start Tika server.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m serverEndpoint\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to start Tika server."
     ]
    }
   ],
   "source": [
    "parser_jd = parser.from_file(\"borneo-JD.txt\")\n",
    "parser_resume = parser.from_file(os.path.join(os.path.dirname(os.getcwd()),\n",
    "\"Training_NER/dataset/test/chenna kesava.docx\"))\n",
    "\n",
    "jd = parser_jd[\"content\"]\n",
    "resume = parser_resume[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Scoring routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    doc = doc.replace(\"•\",\"\")\n",
    "#     doc = doc.replace(\"\")\n",
    "    doc = doc.replace(\"–\",\"\")\n",
    "    doc = doc.replace(\"\\t\",\" \")\n",
    "    doc = doc.strip()\n",
    "    return doc\n",
    "\n",
    "jd = preprocess(jd)\n",
    "resume = preprocess(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_score = 0\n",
    "\n",
    "for word in resume.split(\" \"):\n",
    "    if word.strip() in [\"PhD\",\"PHD\",\"Research Associate\"]:\n",
    "        deg_score=3\n",
    "    elif word.strip() in [\"MS\",\"MT\",\"M.Tech\",\"Masters\"]:\n",
    "        if deg_score<2:\n",
    "            deg_score=2\n",
    "    elif word.strip() in [\"BS\",\"BE\",\"B.S\",\"B.E\",\"B.Tech\",\"Bachelors\"]:\n",
    "        if deg_score<1:\n",
    "            deg_score=1\n",
    "\n",
    "# print(deg_score)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_score = 0\n",
    "\n",
    "for word in resume.split(\" \"):\n",
    "    if word.strip() in [\"Sr.\",\"Senior\"]:\n",
    "        if des_score<3:\n",
    "            des_score=3\n",
    "    elif word.strip() in [\"Associate\", \"Scientist\", \"Engineer\"]:\n",
    "        if des_score<2:\n",
    "            des_score=2\n",
    "    elif word.strip() in [\"Analyst\", \"Junior\"]:\n",
    "        if des_score<1:\n",
    "            des_score=1\n",
    "\n",
    "# print(des_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_score = 0\n",
    "a = re.findall(r'[0-9]+\\+*[ ]?[Yy]ear',resume)\n",
    "a.sort()\n",
    "\n",
    "if len(a)>0:\n",
    "    exp = a[len(a)-1].lower().split(\"y\")[0].strip()\n",
    "#     print(exp)\n",
    "\n",
    "    if \"+\" in exp :\n",
    "        exp = exp[:-1]\n",
    "    \n",
    "    exp = int(exp)\n",
    "#     print(exp)\n",
    "    if exp>=4:\n",
    "        exp_score=3\n",
    "    elif exp>=2:\n",
    "        exp_score=2\n",
    "    elif exp==1:\n",
    "        exp_score=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Document Similarity for comparing resume and skills with the JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinaykumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# splitting the text lines into words\n",
    "# translation table is a global variable\n",
    "# mapping upper case to lower case and\n",
    "# punctuation to spaces\n",
    "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase,\n",
    "\t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase)\n",
    "\t\n",
    "# returns a list of the words\n",
    "# in the file\n",
    "def get_words_from_line_list(text):\n",
    "\t\n",
    "\ttext = text.translate(translation_table)\n",
    "\tword_list = [x for x in text.split() if x not in set(stopwords.words('english'))]\n",
    "\t\n",
    "\treturn word_list\n",
    "\n",
    "\n",
    "# counts frequency of each word\n",
    "# returns a dictionary which maps\n",
    "# the words to their frequency.\n",
    "def count_frequency(word_list):\n",
    "\t\n",
    "\tD = {}\n",
    "\t\n",
    "\tfor new_word in word_list:\n",
    "\t\t\n",
    "\t\tif new_word in D:\n",
    "\t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "\treturn D\n",
    "\n",
    "# returns dictionary of (word, frequency)\n",
    "# pairs from the previous dictionary.\n",
    "def word_frequencies_for_text(text):\n",
    "\t\n",
    "\tline_list = text\n",
    "\tword_list = get_words_from_line_list(line_list)\n",
    "\tfreq_mapping = count_frequency(word_list)\n",
    "\n",
    "# \tprint(\"File\", filename, \":\", )\n",
    "# \tprint(len(line_list), \"lines, \", )\n",
    "# \tprint(len(word_list), \"words, \", )\n",
    "# \tprint(len(freq_mapping), \"distinct words\")\n",
    "\n",
    "\treturn freq_mapping\n",
    "\n",
    "\n",
    "# returns the dot product of two documents\n",
    "def dotProduct(D1, D2):\n",
    "\tSum = 0.0\n",
    "\t\n",
    "\tfor key in D1:\n",
    "\t\t\n",
    "\t\tif key in D2:\n",
    "\t\t\tSum += (D1[key] * D2[key])\n",
    "\t\t\t\n",
    "\treturn Sum\n",
    "\n",
    "# returns the angle in radians\n",
    "# between document vectors\n",
    "def vector_angle(D1, D2):\n",
    "\tnumerator = dotProduct(D1, D2)\n",
    "\tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2))\n",
    "\t\n",
    "\treturn math.acos(numerator / denominator)\n",
    "\n",
    "\n",
    "def documentSimilarity(text_1, text_2):\n",
    "    sorted_word_list_1 = word_frequencies_for_text(text_1)\n",
    "    sorted_word_list_2 = word_frequencies_for_text(text_2)\n",
    "    distance = vector_angle(sorted_word_list_1, sorted_word_list_2)\n",
    "    return math.degrees(distance)\n",
    "\t\n",
    "# Driver code\n",
    "# documentSimilarity(jd, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ner_model(resume)\n",
    "\n",
    "skill_list = [tok.text for tok in doc if tok.ent_type_==\"Skills\"]\n",
    "skill_text = \" \".join(skill_list)\n",
    "\n",
    "skill_score = 0\n",
    "\n",
    "if len(skill_list)>0:\n",
    "    skill_match = 90.0-documentSimilarity(jd,skill_text)\n",
    "    ## Skills are matched on a scale of 0-10\n",
    "    skill_score = min(10,skill_match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Resume Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_match = 90-documentSimilarity(jd, resume)\n",
    "resume_score = min(20,resume_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Score (on scale 1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8\n"
     ]
    }
   ],
   "source": [
    "score = round(10/7*(deg_score*0.20+des_score*0.20+exp_score*0.20+skill_score*0.30+resume_score*0.10),1)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b1b70dabb37e9fb1db1c4f6ebfc24535df5e6d7850a91906a7d79f2982b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
